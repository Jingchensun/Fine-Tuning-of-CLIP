{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demo\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n",
    "print(image.size()) #(1,3,224,224)\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
    "print(text.size()) #(3,77)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image) #(1,512)\n",
    "    print(image_features.size())\n",
    "    text_features = model.encode_text(text) #(3,512)\n",
    "    print(text_features.size())\n",
    "\n",
    "    similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "    similarity2 = image_features.cpu().numpy() @ text_features.cpu().numpy().T\n",
    "    similarity3 = torch.cosine_similarity(text_features, image_features)\n",
    "\n",
    "    print('similarity:',similarity)\n",
    "    print('similarity2:',similarity2)\n",
    "    print('similarity3:',similarity3)\n",
    "\n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    print(logits_per_image.size()) #(1,3)\n",
    "    print(logits_per_text.size())  #(3,1)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "\n",
      "Top predictions:\n",
      "\n",
      "        dinosaur: 50.67%\n",
      "      skyscraper: 25.78%\n",
      "          orange: 7.04%\n",
      "          spider: 5.23%\n",
      "         leopard: 3.11%\n"
     ]
    }
   ],
   "source": [
    "#Zero-Shot Prediction\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR100\n",
    "import joblib\n",
    "from clip.model import Gauss_model\n",
    "\n",
    "#from clip.model import CLIP\n",
    "from torch import nn\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "model_g = Gauss_model().to(device)\n",
    "#model_g = model_g.half()\n",
    "#print(model_g)\n",
    "\n",
    "# Download the dataset\n",
    "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
    "#print(len(cifar100)) #1000\n",
    "\n",
    "# Prepare the inputs\n",
    "image, class_id = cifar100[3637]\n",
    "#print(class_id) #78\n",
    "image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
    "#print(cifar100.classes[class_id]) #snake\n",
    "\n",
    "# Calculate features\n",
    "with torch.no_grad():\n",
    "    # image_features = model.encode_image(image_input)\n",
    "    # text_features = model.encode_text(text_inputs)\n",
    "    image_features,text_features = model_g(image_input,text_inputs)\n",
    "    #image_features,text_features=model_g(image_features,text_features)\n",
    "\n",
    "# Pick the top 5 most similar labels for the image\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "#print(image_features.size()) #torch.Size([1, 512])\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "#print(text_features.size()) #torch.Size([100, 512])\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "values, indices = similarity[0].topk(5)\n",
    "#print('values:',similarity[0].size()) # torch.Size([100])\n",
    "\n",
    "#Print the result\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zero-shot evaluation\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "# Load the dataset\n",
    "root = os.path.expanduser(\"~/.cache\")\n",
    "train = CIFAR100(root, download=True, train=True, transform=preprocess)\n",
    "test = CIFAR100(root, download=True, train=False, transform=preprocess)\n",
    "#print(cifar100.classes)\n",
    "\n",
    "def get_features(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(DataLoader(dataset, batch_size=1000)):\n",
    "            features = model.encode_image(images.to(device))\n",
    "\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "# Calculate the image features\n",
    "train_features, train_labels = get_features(train)\n",
    "test_features, test_labels = get_features(test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Perform logistic regression\n",
    "#classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\n",
    "#classifier.fit(train_features, train_labels)\n",
    "\n",
    "# # Pick the top 5 most similar labels for the image\n",
    "# image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "# text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "# similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "# values, indices = similarity[0].topk(5)\n",
    "\n",
    "# # Print the result\n",
    "# print(\"\\nTop predictions:\\n\")\n",
    "# for value, index in zip(values, indices):\n",
    "#     print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")\n",
    "\n",
    "#from sklearn.externals import joblib\n",
    "#import sklearn.external.joblib as extjoblib\n",
    "# Save to file in the current working directory\n",
    "joblib_file = \"joblib_model.pkl\"\n",
    "#model = classifier\n",
    "#joblib.dump(model, joblib_file)\n",
    "# Load from file\n",
    "joblib_model = joblib.load(joblib_file)\n",
    "# # Calculate the accuracy and predictions\n",
    "# score = joblib_model.score(Xtest, Ytest)\n",
    "# print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "# Ypredict = pickle_model.predict(Xtest)\n",
    "\n",
    "print(joblib_model)\n",
    "classifier= joblib_model\n",
    "# Evaluate using the logistic regression classifier\n",
    "predictions = classifier.predict(test_features) # test_features (10000,512)\n",
    "print('predictions:',len(predictions)) #(10000,)\n",
    "print('test_label:',len(test_labels)) #(10000,)\n",
    "\n",
    "accuracy = np.mean((test_labels == predictions).astype(np.float)) * 100.\n",
    "print(f\"Accuracy = {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear-probe evaluation\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "# Load the dataset\n",
    "root = os.path.expanduser(\"~/.cache\")\n",
    "train = CIFAR100(root, download=True, train=True, transform=preprocess)\n",
    "test = CIFAR100(root, download=True, train=False, transform=preprocess)\n",
    "\n",
    "\n",
    "def get_features(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n",
    "            features = model.encode_image(images.to(device))\n",
    "\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "# Calculate the image features\n",
    "train_features, train_labels = get_features(train)\n",
    "test_features, test_labels = get_features(test)\n",
    "\n",
    "# Perform logistic regression\n",
    "classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Evaluate using the logistic regression classifier\n",
    "predictions = classifier.predict(test_features)\n",
    "accuracy = np.mean((test_labels == predictions).astype(np.float)) * 100.\n",
    "print(f\"Accuracy = {accuracy:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c73bc775c6f94d98a067ce096eff928d580e9c541aafc395dafbb8814a34bdf4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('torch171': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

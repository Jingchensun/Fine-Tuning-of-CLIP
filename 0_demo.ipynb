{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([3, 77])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([3, 512])\n",
      "similarity: [[31.61]\n",
      " [23.81]\n",
      " [22.66]]\n",
      "similarity2: [[31.61 23.81 22.66]]\n",
      "similarity3: tensor([0.2556, 0.2008, 0.1974], device='cuda:3', dtype=torch.float16)\n",
      "torch.Size([1, 3])\n",
      "torch.Size([3, 1])\n",
      "Label probs: [[0.9927   0.00432  0.003061]]\n"
     ]
    }
   ],
   "source": [
    "#demo\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n",
    "print(image.size()) #(1,3,224,224)\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
    "print(text.size()) #(3,77)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image) #(1,512)\n",
    "    print(image_features.size())\n",
    "    text_features = model.encode_text(text) #(3,512)\n",
    "    print(text_features.size())\n",
    "\n",
    "    similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "    similarity2 = image_features.cpu().numpy() @ text_features.cpu().numpy().T\n",
    "    similarity3 = torch.cosine_similarity(text_features, image_features)\n",
    "\n",
    "    print('similarity:',similarity)\n",
    "    print('similarity2:',similarity2)\n",
    "    print('similarity3:',similarity3)\n",
    "\n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    print(logits_per_image.size()) #(1,3)\n",
    "    print(logits_per_text.size())  #(3,1)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 3 does not equal 0 (while checking arguments for cudnn_convolution)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jason/CLIP/0_demo.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/0_demo.ipynb#ch0000001vscode-remote?line=27'>28</a>\u001b[0m \u001b[39m#print(cifar100.classes[class_id]) #snake\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/0_demo.ipynb#ch0000001vscode-remote?line=28'>29</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/0_demo.ipynb#ch0000001vscode-remote?line=29'>30</a>\u001b[0m \u001b[39m# Calculate features\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/0_demo.ipynb#ch0000001vscode-remote?line=30'>31</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/0_demo.ipynb#ch0000001vscode-remote?line=31'>32</a>\u001b[0m     \u001b[39m# image_features = model.encode_image(image_input)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/0_demo.ipynb#ch0000001vscode-remote?line=32'>33</a>\u001b[0m     \u001b[39m# text_features = model.encode_text(text_inputs)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/0_demo.ipynb#ch0000001vscode-remote?line=33'>34</a>\u001b[0m     image_features,text_features \u001b[39m=\u001b[39m model_g(image_input,text_inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/0_demo.ipynb#ch0000001vscode-remote?line=34'>35</a>\u001b[0m     \u001b[39m#image_features,text_features=model_g(image_features,text_features)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/0_demo.ipynb#ch0000001vscode-remote?line=35'>36</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/0_demo.ipynb#ch0000001vscode-remote?line=36'>37</a>\u001b[0m \u001b[39m# Pick the top 5 most similar labels for the image\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/0_demo.ipynb#ch0000001vscode-remote?line=37'>38</a>\u001b[0m image_features \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m image_features\u001b[39m.\u001b[39mnorm(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py?line=724'>725</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py?line=725'>726</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py?line=726'>727</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py?line=727'>728</a>\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py?line=728'>729</a>\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py?line=729'>730</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py?line=730'>731</a>\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/CLIP/clip/model.py:397\u001b[0m, in \u001b[0;36mGauss_model.forward\u001b[0;34m(self, image, text)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jason/CLIP/clip/model.py?line=393'>394</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, image, text):\n\u001b[1;32m    <a href='file:///home/jason/CLIP/clip/model.py?line=394'>395</a>\u001b[0m     \u001b[39m# self.clip = model\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jason/CLIP/clip/model.py?line=395'>396</a>\u001b[0m     \u001b[39m# print(self.clip_model.encode_image(image).size()) #torch.Size([1, 512])\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/jason/CLIP/clip/model.py?line=396'>397</a>\u001b[0m     image_u \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_u(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclip_model\u001b[39m.\u001b[39;49mencode_image(image)\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m    <a href='file:///home/jason/CLIP/clip/model.py?line=397'>398</a>\u001b[0m     \u001b[39m# print(image_u.size())#torch.Size([1, 512])\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jason/CLIP/clip/model.py?line=398'>399</a>\u001b[0m     image_std \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_std(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_model\u001b[39m.\u001b[39mencode_image(image)\u001b[39m.\u001b[39mfloat())\n",
      "File \u001b[0;32m~/CLIP/clip/model.py:338\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jason/CLIP/clip/model.py?line=336'>337</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_image\u001b[39m(\u001b[39mself\u001b[39m, image):\n\u001b[0;32m--> <a href='file:///home/jason/CLIP/clip/model.py?line=337'>338</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisual(image\u001b[39m.\u001b[39;49mtype(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype))\n",
      "File \u001b[0;32m~/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py?line=724'>725</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py?line=725'>726</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py?line=726'>727</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py?line=727'>728</a>\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py?line=728'>729</a>\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py?line=729'>730</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py?line=730'>731</a>\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/CLIP/clip/model.py:221\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jason/CLIP/clip/model.py?line=219'>220</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> <a href='file:///home/jason/CLIP/clip/model.py?line=220'>221</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)  \u001b[39m# shape = [*, width, grid, grid]\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jason/CLIP/clip/model.py?line=221'>222</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreshape(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# shape = [*, width, grid ** 2]\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jason/CLIP/clip/model.py?line=222'>223</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# shape = [*, grid ** 2, width]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py?line=724'>725</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py?line=725'>726</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py?line=726'>727</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py?line=727'>728</a>\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py?line=728'>729</a>\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py?line=729'>730</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/module.py?line=730'>731</a>\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/conv.py:423\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/conv.py?line=421'>422</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/conv.py?line=422'>423</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/conv.py:419\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/conv.py?line=414'>415</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/conv.py?line=415'>416</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/conv.py?line=416'>417</a>\u001b[0m                     weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/conv.py?line=417'>418</a>\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/conv.py?line=418'>419</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/nn/modules/conv.py?line=419'>420</a>\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 3 does not equal 0 (while checking arguments for cudnn_convolution)"
     ]
    }
   ],
   "source": [
    "#Zero-Shot Prediction\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR100\n",
    "import joblib\n",
    "from clip.model import Gauss_model\n",
    "\n",
    "#from clip.model import CLIP\n",
    "from torch import nn\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "model_g = Gauss_model().to(device)\n",
    "#model_g = model_g.half()\n",
    "#print(model_g)\n",
    "\n",
    "# Download the dataset\n",
    "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
    "#print(len(cifar100)) #1000\n",
    "\n",
    "# Prepare the inputs\n",
    "image, class_id = cifar100[3637]\n",
    "#print(class_id) #78\n",
    "image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
    "#print(cifar100.classes[class_id]) #snake\n",
    "\n",
    "# Calculate features\n",
    "with torch.no_grad():\n",
    "    # image_features = model.encode_image(image_input)\n",
    "    # text_features = model.encode_text(text_inputs)\n",
    "    image_features,text_features = model_g(image_input,text_inputs)\n",
    "    #image_features,text_features=model_g(image_features,text_features)\n",
    "\n",
    "# Pick the top 5 most similar labels for the image\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "#print(image_features.size()) #torch.Size([1, 512])\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "#print(text_features.size()) #torch.Size([100, 512])\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "values, indices = similarity[0].topk(5)\n",
    "#print('values:',similarity[0].size()) # torch.Size([100])\n",
    "\n",
    "#Print the result\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zero-shot evaluation\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "# Load the dataset\n",
    "root = os.path.expanduser(\"~/.cache\")\n",
    "train = CIFAR100(root, download=True, train=True, transform=preprocess)\n",
    "test = CIFAR100(root, download=True, train=False, transform=preprocess)\n",
    "#print(cifar100.classes)\n",
    "\n",
    "def get_features(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(DataLoader(dataset, batch_size=1000)):\n",
    "            features = model.encode_image(images.to(device))\n",
    "\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "# Calculate the image features\n",
    "train_features, train_labels = get_features(train)\n",
    "test_features, test_labels = get_features(test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Perform logistic regression\n",
    "#classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\n",
    "#classifier.fit(train_features, train_labels)\n",
    "\n",
    "# # Pick the top 5 most similar labels for the image\n",
    "# image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "# text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "# similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "# values, indices = similarity[0].topk(5)\n",
    "\n",
    "# # Print the result\n",
    "# print(\"\\nTop predictions:\\n\")\n",
    "# for value, index in zip(values, indices):\n",
    "#     print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")\n",
    "\n",
    "#from sklearn.externals import joblib\n",
    "#import sklearn.external.joblib as extjoblib\n",
    "# Save to file in the current working directory\n",
    "joblib_file = \"joblib_model.pkl\"\n",
    "#model = classifier\n",
    "#joblib.dump(model, joblib_file)\n",
    "# Load from file\n",
    "joblib_model = joblib.load(joblib_file)\n",
    "# # Calculate the accuracy and predictions\n",
    "# score = joblib_model.score(Xtest, Ytest)\n",
    "# print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "# Ypredict = pickle_model.predict(Xtest)\n",
    "\n",
    "print(joblib_model)\n",
    "classifier= joblib_model\n",
    "# Evaluate using the logistic regression classifier\n",
    "predictions = classifier.predict(test_features) # test_features (10000,512)\n",
    "print('predictions:',len(predictions)) #(10000,)\n",
    "print('test_label:',len(test_labels)) #(10000,)\n",
    "\n",
    "accuracy = np.mean((test_labels == predictions).astype(np.float)) * 100.\n",
    "print(f\"Accuracy = {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear-probe evaluation\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "# Load the dataset\n",
    "root = os.path.expanduser(\"~/.cache\")\n",
    "train = CIFAR100(root, download=True, train=True, transform=preprocess)\n",
    "test = CIFAR100(root, download=True, train=False, transform=preprocess)\n",
    "\n",
    "\n",
    "def get_features(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n",
    "            features = model.encode_image(images.to(device))\n",
    "\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "# Calculate the image features\n",
    "train_features, train_labels = get_features(train)\n",
    "test_features, test_labels = get_features(test)\n",
    "\n",
    "# Perform logistic regression\n",
    "classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Evaluate using the logistic regression classifier\n",
    "predictions = classifier.predict(test_features)\n",
    "accuracy = np.mean((test_labels == predictions).astype(np.float)) * 100.\n",
    "print(f\"Accuracy = {accuracy:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c73bc775c6f94d98a067ce096eff928d580e9c541aafc395dafbb8814a34bdf4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('torch171': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "image_path='/home/jason/data/coco/images/'\n",
    "mode='train2014/'\n",
    "image_list=[]\n",
    "image_list.extend(glob.glob(os.path.join(image_path,mode, '*.jpg')))\n",
    "image_list.sort()\n",
    "print(len(image_list))\n",
    "\n",
    "text_path='/home/jason/data/coco/text/'\n",
    "label_list = []\n",
    "label_list.extend(glob.glob(os.path.join(text_path,mode, '*.txt')))\n",
    "label_list.sort()\n",
    "print(len(label_list))\n",
    "\n",
    "with open(label_list[0], \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    label = random.choice(data)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/openai/CLIP/issues/57\n",
    "# old fine-tuned based clip\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "EPOCH =10\n",
    "BATCH_SIZE =32\n",
    "\n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "class cocodtrain(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_path='/home/jason/data/coco/images', text_path='/home/jason/data/coco/text', mode='train2014'):\n",
    "\n",
    "        self.image_list = []\n",
    "        self.image_list.extend(glob.glob(os.path.join(image_path, mode, '*.jpg')))\n",
    "        self.image_list.sort()\n",
    "\n",
    "        self.label_list = []\n",
    "        self.label_list.extend(glob.glob(os.path.join(text_path, mode, '*.txt')))\n",
    "        self.label_list.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_list[index]).convert(\"RGB\")\n",
    "        image = image.resize((224,224), Image.BILINEAR)\n",
    "        image = preprocess(image)\n",
    "        #image = np.asarray(image)\n",
    "\n",
    "        with open(self.label_list[index], \"r\") as f:\n",
    "            data = f.readlines()\n",
    "            label = random.choice(data)\n",
    "            \n",
    "        return image, label\n",
    "trainset = cocodtrain('/home/jason/data/coco/images','/home/jason/data/coco/text','train2014')\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "                    trainset, \n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=True, \n",
    "                    num_workers=16,\n",
    "                    drop_last=True)\n",
    "\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "#device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "#model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "#clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    print('epoch:', epoch)\n",
    "    for batch in tqdm(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        list_image,list_txt = batch #list_images is list of image in numpy array(np.uint8), or list of PIL images\n",
    "        # print(list_image.size()) #torch.Size([32, 3, 224, 224])\n",
    "        # print(len(list_txt)) #32\n",
    "      \n",
    "        images = torch.tensor(np.stack(list_image)).to(device)\n",
    "        texts = clip.tokenize(list_txt).to(device) #torch.Size([32, 77])\n",
    "         # print(texts.size()) #torch.Size([32, 77])\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "        #print(logits_per_image.size(),logits_per_text.size()) #torch.Size([32, 32])  torch.Size([32, 32]) \n",
    "        ground_truth = torch.arange(BATCH_SIZE,dtype=torch.long,device=device)\n",
    "        #print('ground.size()',ground_truth.size()) #torch.Size([32])\n",
    "\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        print('total_loss',total_loss)\n",
    "        total_loss.backward()\n",
    "      \n",
    "        convert_models_to_fp32(model)\n",
    "        optimizer.step()\n",
    "        clip.model.convert_weights(model)\n",
    "    \n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': total_loss,\n",
    "    }, f\"model_checkpoint/model_10.pt\") #just change to your preferred folder/filename      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "checkpoint = torch.load(\"model_checkpoint/model_10.pt\")\n",
    "#print(model.input_resolution)\n",
    "# Use these 3 lines if you use default model setting(not training setting) of the clip. For example, if you set context_length to 100 since your string is very long during training, then assign 100 to checkpoint['model_state_dict'][\"context_length\"] \n",
    "# checkpoint['model_state_dict'][\"input_resolution\"] = model.input_resolution #default is 224\n",
    "# checkpoint['model_state_dict'][\"context_length\"] = model.context_length # default is 77\n",
    "# checkpoint['model_state_dict'][\"vocab_size\"] = model.vocab_size \n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new numpy based clip\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from clip.model import Gauss_model\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "model_g = Gauss_model().to(device)\n",
    "_, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "\n",
    "EPOCH =10\n",
    "BATCH_SIZE =32\n",
    "\n",
    "class cocodtrain(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_path='/home/jason/data/coco/images', text_path='/home/jason/data/coco/text', mode='train2014'):\n",
    "\n",
    "        self.image_list = []\n",
    "        self.image_list.extend(glob.glob(os.path.join(image_path, mode, '*.jpg')))\n",
    "        self.image_list.sort()\n",
    "\n",
    "        self.label_list = []\n",
    "        self.label_list.extend(glob.glob(os.path.join(text_path, mode, '*.txt')))\n",
    "        self.label_list.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_list[index]).convert(\"RGB\")\n",
    "        image = image.resize((224,224), Image.BILINEAR)\n",
    "        image = preprocess(image)\n",
    "        #image = np.asarray(image)\n",
    "\n",
    "        with open(self.label_list[index], \"r\") as f:\n",
    "            data = f.readlines()\n",
    "            label = random.choice(data)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "trainset = cocodtrain('/home/jason/data/coco/images','/home/jason/data/coco/text','train2014')\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "                    trainset, \n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=True, \n",
    "                    num_workers=0,\n",
    "                    drop_last=True)\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_g.parameters(), lr=5e-6,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "\n",
    "def loss_multivariate_normal_kl_multi(mu_1, sigma_1, mu_2, sigma_2):\n",
    "    b, dim = mu_1.shape\n",
    "    #print(b,dim)\n",
    "    mu_1 = mu_1.unsqueeze(1).unsqueeze(2)\n",
    "    sigmasquare_1 = sigma_1.unsqueeze(1)\n",
    "    mu_2 = mu_2.unsqueeze(0).unsqueeze(2)\n",
    "    sigmasquare_2 =sigma_2.unsqueeze(0)\n",
    "\n",
    "    p1 = torch.prod(sigmasquare_1,-1)\n",
    "    p2 = torch.prod(sigmasquare_2,-1)\n",
    "    if torch.any(p1 == 0) or torch.any(p2 == 0):\n",
    "        first = 0\n",
    "        #print('p1=======0')\n",
    "    else:\n",
    "        first = p2.log() - p1.log()\n",
    "\n",
    "    #first = torch.log(torch.prod(sigmasquare_2,-1)/torch.prod(sigmasquare_1,-1))\n",
    "    #print('first',first)\n",
    "    second = -dim\n",
    "    #print('second',second)\n",
    "    third = ((sigmasquare_1 / sigmasquare_2)).sum(-1)\n",
    "    #print('third',third)\n",
    "    a = mu_1 - mu_2\n",
    "    b = torch.matmul(a, torch.diag_embed(1/sigmasquare_2))\n",
    "    c = torch.transpose(a,3,2)\n",
    "    fourth = torch.matmul(b, c).squeeze(3).squeeze(2)\n",
    "    \n",
    "    kl = 0.5 * (first + second + third + fourth)\n",
    "    return kl\n",
    "\n",
    "n = BATCH_SIZE\n",
    "Kl_matric = np.ones([n,n])\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    print('epoch:', epoch)\n",
    "    for batch in tqdm(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        list_image,list_txt = batch #list_images is list of image in numpy array(np.uint8), or list of PIL images\n",
    "        # print(list_image.size()) #torch.Size([32, 3, 224, 224])\n",
    "        #print(len(list_txt))\n",
    "        images = torch.tensor(np.stack(list_image),requires_grad=True).to(device)\n",
    "        #print('image size:',images.size()) #image size: torch.Size([32, 3, 224, 224])\n",
    "        texts = clip.tokenize(list_txt).to(device) #torch.Size([32, 77])\n",
    "        #  print(texts.size()) #torch.Size([32, 77])\n",
    "        image_u,image_std,text_u,text_std= model_g(images, texts)\n",
    "        #print(image_u.size()) #torch.Size([32, 512])\n",
    "\n",
    "        p_q_kl = loss_multivariate_normal_kl_multi(image_u,image_std,text_u,text_std)\n",
    "        #print(p_q_kl.size())\n",
    "        # print(Kl_matric.requires_grad)\n",
    "        logits_per_image = torch.tensor(p_q_kl,requires_grad=True).to(device)\n",
    "        logits_per_text = logits_per_image.t()\n",
    "        #print(logits_per_image.size(),logits_per_text.size())\n",
    "\n",
    "        ground_truth = torch.arange(BATCH_SIZE,dtype=torch.long,device=device)\n",
    "        #print(ground_truth.size())\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        total_loss.backward()\n",
    "        print('total loss:', total_loss)\n",
    "      \n",
    "        #convert_models_to_fp32(model)\n",
    "        optimizer.step()\n",
    "        #clip.model.convert_weights(model)\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model_g.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': total_loss,\n",
    "    }, f\"model_checkpoint/model_10.pt\") #just change to your preferred folder/filename   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-KL distributions\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "mu1 = torch.tensor([0.2, 0.1, 0.5, 0.4]).unsqueeze(0)\n",
    "Sigma1= torch.tensor([0.14, 0.52, 0.2, 0.4]).unsqueeze(0)\n",
    "\n",
    "mu2 = torch.tensor([0.3, 0.6, -0.5, -0.8]).unsqueeze(0)\n",
    "Sigma2 = torch.tensor([0.24, 0.02, 0.31, 0.51]).unsqueeze(0)\n",
    "\n",
    "\n",
    "#Method 1 single\n",
    "def loss_multivariate_normal_kl_single(mu_1, sigmasquare_1, mu_2, sigmasquare_2):\n",
    "    b, dim = mu_1.shape\n",
    "    kl = 0\n",
    "\n",
    "    for bidx in range(b):\n",
    "            c_mu_1 = mu_1[bidx]\n",
    "            c_Sigma_1 = torch.diag(sigmasquare_1[bidx])\n",
    "\n",
    "            c_mu_2 = mu_2[bidx]\n",
    "            c_Sigma_2 = torch.diag(sigmasquare_2[bidx])\n",
    "\n",
    "            # p1 = c_Sigma_1.det()\n",
    "            # p2 = c_Sigma_2.det()\n",
    "            # if p1 == 0 or p2 ==0:\n",
    "            #     first = 0\n",
    "            #     print('p1=======0')\n",
    "            # else:\n",
    "            #     first = p1.log() - p2.log()\n",
    "\n",
    "            first = c_Sigma_2.det().log() - c_Sigma_1.det().log()\n",
    "            second = -dim\n",
    "            third = torch.matmul(c_Sigma_2.inverse(), c_Sigma_1).trace()\n",
    "            fourth = torch.matmul(torch.matmul((c_mu_2 - c_mu_1).T, c_Sigma_2.inverse()), c_mu_2 - c_mu_1)\n",
    "            kl = kl + 0.5 * (first + second + third + fourth)\n",
    "            #print(kl)\n",
    "    kl = kl/b\n",
    "    return kl\n",
    "print(loss_multivariate_normal_kl_single(mu1,Sigma1,mu2,Sigma2))\n",
    "\n",
    "p_distribution = torch.distributions.MultivariateNormal(mu1, torch.diag_embed(Sigma1))\n",
    "q_distribution = torch.distributions.MultivariateNormal(mu2, torch.diag_embed(Sigma2))\n",
    "p_q_kl = torch.distributions.kl_divergence(p_distribution, q_distribution)#.mean()\n",
    "\n",
    "print(p_q_kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muti-KL distributions\n",
    "import torch\n",
    "import numpy as np\n",
    "n = 4 #batch size\n",
    "\n",
    "mu1 = torch.rand(n, 512)\n",
    "print(mu1.shape)\n",
    "Sigma1=torch.rand(n, 512)\n",
    "\n",
    "mu2 = torch.rand(n,512)\n",
    "Sigma2=torch.rand(n,512)\n",
    "\n",
    "# mu1 = torch.tensor([[0.2, 0.1, 0.5, 0.4],[0.14, 0.52, 0.2, 0.4]])#.unsqueeze(0)\n",
    "# #print(mu1.shape)#torch.Size([2, 4])\n",
    "# Sigma1= torch.tensor([[0.14, 0.52, 0.2, 0.4],[0.2, 0.1, 0.5, 0.4]])#.unsqueeze(0)\n",
    "\n",
    "# mu2 = torch.tensor([[0.3, 0.6, -0.5, -0.8],[0.24, 0.02, 0.31, 0.51]])#.unsqueeze(0)\n",
    "# Sigma2 = torch.tensor([[0.24, 0.02, 0.31, 0.51],[0.24, 0.02, 0.31, 0.51]])#.unsqueeze(0)\n",
    "\n",
    "\n",
    "#Method 2 two-loop\n",
    "def loss_multivariate_normal_kl_loop(mu_1, sigmasquare_1, mu_2, sigmasquare_2):\n",
    "    b, dim = mu_1.shape\n",
    "    kl = 0\n",
    "    kkll = torch.ones(b,b)\n",
    "    for x in range(b):\n",
    "        for y in range(b):\n",
    "            #print(x,y)\n",
    "            c_mu_1 = mu_1[x]\n",
    "            c_Sigma_1 = torch.diag(sigmasquare_1[x])\n",
    "            c_mu_2 = mu_2[y]\n",
    "            c_Sigma_2 = torch.diag(sigmasquare_2[y])\n",
    "\n",
    "            p1 = c_Sigma_1.det()\n",
    "            p2 = c_Sigma_2.det()\n",
    "            if p1 == 0 or p2 ==0:\n",
    "                first = 0\n",
    "                print('p1=======0')\n",
    "            else:\n",
    "                first = p2.log() - p1.log()\n",
    "\n",
    "            #first = c_Sigma_2.det().log() - c_Sigma_1.det().log()\n",
    "            #print('first',first)\n",
    "            second = -dim\n",
    "            #print('second',second)\n",
    "            third = torch.matmul(c_Sigma_2.inverse(), c_Sigma_1).trace()\n",
    "            #print('third',third)\n",
    "            fourth = torch.matmul(torch.matmul((c_mu_2 - c_mu_1).T, c_Sigma_2.inverse()), c_mu_2 - c_mu_1)\n",
    "            #print('fourth',fourth)\n",
    "            kl = 0.5 * (first + second + third + fourth)\n",
    "            #print('kl',kl)\n",
    "            kkll[x,y] = kl\n",
    "    return kkll\n",
    "\n",
    "#Method3 multi dimenson\n",
    "def loss_multivariate_normal_kl_multi(mu_1, sigma_1, mu_2, sigma_2):\n",
    "    b, dim = mu_1.shape\n",
    "    #print(b,dim)\n",
    "    mu_1 = mu_1.unsqueeze(1).unsqueeze(2)\n",
    "    sigmasquare_1 = sigma_1.unsqueeze(1)\n",
    "    mu_2 = mu_2.unsqueeze(0).unsqueeze(2)\n",
    "    sigmasquare_2 =sigma_2.unsqueeze(0)\n",
    "\n",
    "    p1 = torch.prod(sigmasquare_1,-1)\n",
    "    p2 = torch.prod(sigmasquare_2,-1)\n",
    "    if torch.any(p1 == 0) or torch.any(p2 == 0):\n",
    "        first = 0\n",
    "        print('p1=======0')\n",
    "    else:\n",
    "        first = p2.log() - p1.log()\n",
    "\n",
    "    #first = torch.log(torch.prod(sigmasquare_2,-1)/torch.prod(sigmasquare_1,-1))\n",
    "    #print('first',first)\n",
    "    second = -dim\n",
    "    #print('second',second)\n",
    "    third = ((sigmasquare_1 / sigmasquare_2)).sum(-1)\n",
    "    #print('third',third)\n",
    "    a = mu_1 - mu_2\n",
    "    b = torch.matmul(a, torch.diag_embed(1/sigmasquare_2))\n",
    "    c = torch.transpose(a,3,2)\n",
    "    fourth = torch.matmul(b, c).squeeze(3).squeeze(2)\n",
    "    \n",
    "    kl = 0.5 * (first + second + third + fourth)\n",
    "    return kl\n",
    "\n",
    "print(loss_multivariate_normal_kl_multi(mu1,Sigma1,mu2,Sigma2))\n",
    "\n",
    "# Method4 distributions\n",
    "mu1=mu1.unsqueeze(1)\n",
    "Sigma1=Sigma1.unsqueeze(1)\n",
    "mu2=mu2.unsqueeze(0)\n",
    "Sigma2=Sigma2.unsqueeze(1)\n",
    "\n",
    "p_distribution = torch.distributions.MultivariateNormal(mu1, torch.diag_embed(Sigma1))\n",
    "q_distribution = torch.distributions.MultivariateNormal(mu2, torch.diag_embed(Sigma2))\n",
    "p_q_kl = torch.distributions.kl_divergence(p_distribution, q_distribution)#.mean()\n",
    "\n",
    "print(p_q_kl)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c73bc775c6f94d98a067ce096eff928d580e9c541aafc395dafbb8814a34bdf4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('torch171': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

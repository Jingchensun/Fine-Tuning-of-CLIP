{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "image_path='/home/jason/data/coco/images/'\n",
    "mode='train2014/'\n",
    "image_list=[]\n",
    "image_list.extend(glob.glob(os.path.join(image_path,mode, '*.jpg')))\n",
    "image_list.sort()\n",
    "print(len(image_list))\n",
    "\n",
    "text_path='/home/jason/data/coco/text/'\n",
    "label_list = []\n",
    "label_list.extend(glob.glob(os.path.join(text_path,mode, '*.txt')))\n",
    "label_list.sort()\n",
    "print(len(label_list))\n",
    "\n",
    "with open(label_list[0], \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    label = random.choice(data)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/openai/CLIP/issues/57\n",
    "# old fine-tuned based clip\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "EPOCH =10\n",
    "BATCH_SIZE =32\n",
    "\n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "class cocodtrain(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_path='/home/jason/data/coco/images', text_path='/home/jason/data/coco/text', mode='train2014'):\n",
    "\n",
    "        self.image_list = []\n",
    "        self.image_list.extend(glob.glob(os.path.join(image_path, mode, '*.jpg')))\n",
    "        self.image_list.sort()\n",
    "\n",
    "        self.label_list = []\n",
    "        self.label_list.extend(glob.glob(os.path.join(text_path, mode, '*.txt')))\n",
    "        self.label_list.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_list[index]).convert(\"RGB\")\n",
    "        image = image.resize((224,224), Image.BILINEAR)\n",
    "        image = preprocess(image)\n",
    "        #image = np.asarray(image)\n",
    "\n",
    "        with open(self.label_list[index], \"r\") as f:\n",
    "            data = f.readlines()\n",
    "            label = random.choice(data)\n",
    "            \n",
    "        return image, label\n",
    "trainset = cocodtrain('/home/jason/data/coco/images','/home/jason/data/coco/text','train2014')\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "                    trainset, \n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=True, \n",
    "                    num_workers=16,\n",
    "                    drop_last=True)\n",
    "\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "#device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "#model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "#clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    print('epoch:', epoch)\n",
    "    for batch in tqdm(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        list_image,list_txt = batch #list_images is list of image in numpy array(np.uint8), or list of PIL images\n",
    "        # print(list_image.size()) #torch.Size([32, 3, 224, 224])\n",
    "        # print(len(list_txt)) #32\n",
    "      \n",
    "        images = torch.tensor(np.stack(list_image)).to(device)\n",
    "        texts = clip.tokenize(list_txt).to(device) #torch.Size([32, 77])\n",
    "         # print(texts.size()) #torch.Size([32, 77])\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "        #print(logits_per_image.size(),logits_per_text.size()) #torch.Size([32, 32])  torch.Size([32, 32]) \n",
    "        ground_truth = torch.arange(BATCH_SIZE,dtype=torch.long,device=device)\n",
    "        #print('ground.size()',ground_truth.size()) #torch.Size([32])\n",
    "\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        print('total_loss',total_loss)\n",
    "        total_loss.backward()\n",
    "      \n",
    "        convert_models_to_fp32(model)\n",
    "        optimizer.step()\n",
    "        clip.model.convert_weights(model)\n",
    "    \n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': total_loss,\n",
    "    }, f\"model_checkpoint/model_10.pt\") #just change to your preferred folder/filename      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "checkpoint = torch.load(\"model_checkpoint/model_10.pt\")\n",
    "#print(model.input_resolution)\n",
    "# Use these 3 lines if you use default model setting(not training setting) of the clip. For example, if you set context_length to 100 since your string is very long during training, then assign 100 to checkpoint['model_state_dict'][\"context_length\"] \n",
    "# checkpoint['model_state_dict'][\"input_resolution\"] = model.input_resolution #default is 224\n",
    "# checkpoint['model_state_dict'][\"context_length\"] = model.context_length # default is 77\n",
    "# checkpoint['model_state_dict'][\"vocab_size\"] = model.vocab_size \n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2bd39d9dfe14aec95c7a5bb470ac713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "cholesky_cuda: U(1,1) is zero, singular U.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jason/CLIP/3_finetune.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000003vscode-remote?line=83'>84</a>\u001b[0m (mu1, Sigma1) \u001b[39m=\u001b[39m  image_u[i], image_std[i]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000003vscode-remote?line=84'>85</a>\u001b[0m (mu2, Sigma2) \u001b[39m=\u001b[39m text_u[j], text_std[j]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000003vscode-remote?line=86'>87</a>\u001b[0m p_distribution \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mdistributions\u001b[39m.\u001b[39;49mMultivariateNormal(mu1, torch\u001b[39m.\u001b[39;49mdiag_embed(Sigma1))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000003vscode-remote?line=87'>88</a>\u001b[0m q_distribution \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdistributions\u001b[39m.\u001b[39mMultivariateNormal(mu2, torch\u001b[39m.\u001b[39mdiag_embed(Sigma2))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000003vscode-remote?line=88'>89</a>\u001b[0m Kl_matric[i,j]  \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdistributions\u001b[39m.\u001b[39mkl_divergence(p_distribution, q_distribution)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/distributions/multivariate_normal.py:149\u001b[0m, in \u001b[0;36mMultivariateNormal.__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unbroadcasted_scale_tril \u001b[39m=\u001b[39m scale_tril\n\u001b[1;32m    148\u001b[0m \u001b[39melif\u001b[39;00m covariance_matrix \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unbroadcasted_scale_tril \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcholesky(covariance_matrix)\n\u001b[1;32m    150\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# precision_matrix is not None\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unbroadcasted_scale_tril \u001b[39m=\u001b[39m _precision_to_scale_tril(precision_matrix)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cholesky_cuda: U(1,1) is zero, singular U."
     ]
    }
   ],
   "source": [
    "# new torch based clip\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from clip.model import Gauss_model\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "model_g = Gauss_model().to(device)\n",
    "_, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "\n",
    "EPOCH =10\n",
    "BATCH_SIZE =32\n",
    "\n",
    "class cocodtrain(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_path='/home/jason/data/coco/images', text_path='/home/jason/data/coco/text', mode='train2014'):\n",
    "\n",
    "        self.image_list = []\n",
    "        self.image_list.extend(glob.glob(os.path.join(image_path, mode, '*.jpg')))\n",
    "        self.image_list.sort()\n",
    "\n",
    "        self.label_list = []\n",
    "        self.label_list.extend(glob.glob(os.path.join(text_path, mode, '*.txt')))\n",
    "        self.label_list.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_list[index]).convert(\"RGB\")\n",
    "        image = image.resize((224,224), Image.BILINEAR)\n",
    "        image = preprocess(image)\n",
    "        #image = np.asarray(image)\n",
    "\n",
    "        with open(self.label_list[index], \"r\") as f:\n",
    "            data = f.readlines()\n",
    "            label = random.choice(data)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "trainset = cocodtrain('/home/jason/data/coco/images','/home/jason/data/coco/text','train2014')\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "                    trainset, \n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=True, \n",
    "                    num_workers=0,\n",
    "                    drop_last=True)\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_g.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "n = BATCH_SIZE\n",
    "Kl_matric = torch.ones([n,n])\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    print('epoch:', epoch)\n",
    "    for batch in tqdm(trainloader):\n",
    "        \n",
    "        list_image,list_txt = batch #list_images is list of image in numpy array(np.uint8), or list of PIL images\n",
    "        # print(list_image.size()) #torch.Size([32, 3, 224, 224])\n",
    "        #print(len(list_txt))\n",
    "        images = torch.tensor(np.stack(list_image),requires_grad=True).to(device)\n",
    "        #print('image size:',images.size()) #image size: torch.Size([32, 3, 224, 224])\n",
    "        texts = clip.tokenize(list_txt).to(device) #torch.Size([32, 77])\n",
    "        #  print(texts.size()) #torch.Size([32, 77])\n",
    "        image_u,image_std,text_u,text_std= model_g(images, texts)\n",
    "        #print(image_u.size()) #torch.Size([32, 512])\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                (mu1, Sigma1) =  image_u[i], image_std[i]\n",
    "                (mu2, Sigma2) = text_u[j], text_std[j]\n",
    "\n",
    "                p_distribution = torch.distributions.MultivariateNormal(mu1, torch.diag_embed(Sigma1))\n",
    "                q_distribution = torch.distributions.MultivariateNormal(mu2, torch.diag_embed(Sigma2))\n",
    "                Kl_matric[i,j]  = torch.distributions.kl_divergence(p_distribution, q_distribution)\n",
    "        \n",
    "        # print(Kl_matric)\n",
    "        #Kl_matric.requires_grad=True\n",
    "        logits_per_image = Kl_matric.to(device)\n",
    "        logits_per_text = logits_per_image.t()\n",
    "        print(logits_per_image.size(),logits_per_text.size())\n",
    "\n",
    "        ground_truth = torch.arange(BATCH_SIZE,dtype=torch.long,device=device)\n",
    "        print(ground_truth.size())\n",
    "        total_loss_ = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_loss_.backward(retain_graph=True)\n",
    "        print('total loss:', total_loss_)\n",
    "      \n",
    "        #convert_models_to_fp32(model)\n",
    "        optimizer.step()\n",
    "        #clip.model.convert_weights(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b2f6e9628f440fb31191ef5ab5b406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_353399/764092385.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  logits_per_image = torch.tensor(p_q_kl,requires_grad=True).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss: tensor(89338.2500, device='cuda:3', grad_fn=<DivBackward0>)\n",
      "total loss: tensor(13404.0459, device='cuda:3', grad_fn=<DivBackward0>)\n",
      "total loss: tensor(1953.9993, device='cuda:3', grad_fn=<DivBackward0>)\n",
      "total loss: tensor(2303.8918, device='cuda:3', grad_fn=<DivBackward0>)\n",
      "total loss: tensor(6578.2305, device='cuda:3', grad_fn=<DivBackward0>)\n",
      "total loss: tensor(2087.3604, device='cuda:3', grad_fn=<DivBackward0>)\n",
      "total loss: tensor(8010.4668, device='cuda:3', grad_fn=<DivBackward0>)\n",
      "total loss: tensor(65556.3047, device='cuda:3', grad_fn=<DivBackward0>)\n",
      "total loss: tensor(3617.5542, device='cuda:3', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jason/CLIP/3_finetune.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 96>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000004vscode-remote?line=106'>107</a>\u001b[0m image_u,image_std,text_u,text_std\u001b[39m=\u001b[39m model_g(images, texts)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000004vscode-remote?line=107'>108</a>\u001b[0m \u001b[39m#print(image_u.size()) #torch.Size([32, 512])\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000004vscode-remote?line=109'>110</a>\u001b[0m p_q_kl \u001b[39m=\u001b[39m loss_multivariate_normal_kl2(image_u,image_std,text_u,text_std)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000004vscode-remote?line=110'>111</a>\u001b[0m \u001b[39m#print(p_q_kl.size())\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000004vscode-remote?line=111'>112</a>\u001b[0m \u001b[39m# print(Kl_matric.requires_grad)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000004vscode-remote?line=112'>113</a>\u001b[0m logits_per_image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(p_q_kl,requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m/home/jason/CLIP/3_finetune.ipynb Cell 5'\u001b[0m in \u001b[0;36mloss_multivariate_normal_kl2\u001b[0;34m(mu_1, sigmasquare_1, mu_2, sigmasquare_2)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000004vscode-remote?line=84'>85</a>\u001b[0m \u001b[39m#first = c_Sigma_2.det().log() - c_Sigma_1.det().log()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000004vscode-remote?line=85'>86</a>\u001b[0m second \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mdim\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000004vscode-remote?line=86'>87</a>\u001b[0m third \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(c_Sigma_2\u001b[39m.\u001b[39;49minverse(), c_Sigma_1)\u001b[39m.\u001b[39mtrace()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000004vscode-remote?line=87'>88</a>\u001b[0m fourth \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(torch\u001b[39m.\u001b[39mmatmul((c_mu_2 \u001b[39m-\u001b[39m c_mu_1)\u001b[39m.\u001b[39mT, c_Sigma_2\u001b[39m.\u001b[39minverse()), c_mu_2 \u001b[39m-\u001b[39m c_mu_1)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000004vscode-remote?line=88'>89</a>\u001b[0m kl \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m (first \u001b[39m+\u001b[39m second \u001b[39m+\u001b[39m third \u001b[39m+\u001b[39m fourth)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#new numpy based clip\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from clip.model import Gauss_model\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "model_g = Gauss_model().to(device)\n",
    "_, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "\n",
    "EPOCH =10\n",
    "BATCH_SIZE =32\n",
    "\n",
    "class cocodtrain(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_path='/home/jason/data/coco/images', text_path='/home/jason/data/coco/text', mode='train2014'):\n",
    "\n",
    "        self.image_list = []\n",
    "        self.image_list.extend(glob.glob(os.path.join(image_path, mode, '*.jpg')))\n",
    "        self.image_list.sort()\n",
    "\n",
    "        self.label_list = []\n",
    "        self.label_list.extend(glob.glob(os.path.join(text_path, mode, '*.txt')))\n",
    "        self.label_list.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_list[index]).convert(\"RGB\")\n",
    "        image = image.resize((224,224), Image.BILINEAR)\n",
    "        image = preprocess(image)\n",
    "        #image = np.asarray(image)\n",
    "\n",
    "        with open(self.label_list[index], \"r\") as f:\n",
    "            data = f.readlines()\n",
    "            label = random.choice(data)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "trainset = cocodtrain('/home/jason/data/coco/images','/home/jason/data/coco/text','train2014')\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "                    trainset, \n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=True, \n",
    "                    num_workers=0,\n",
    "                    drop_last=True)\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_g.parameters(), lr=5e-6,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "\n",
    "def loss_multivariate_normal_kl2(mu_1, sigmasquare_1, mu_2, sigmasquare_2):\n",
    "    b, dim = mu_1.shape\n",
    "    kl = 0\n",
    "    kkll = torch.ones(b,b)\n",
    "    for x in range(b):\n",
    "        for y in range(b):\n",
    "            c_mu_1 = mu_1[x]\n",
    "            c_Sigma_1 = torch.diag(sigmasquare_1[x])\n",
    "\n",
    "            c_mu_2 = mu_2[y]\n",
    "            c_Sigma_2 = torch.diag(sigmasquare_2[y])\n",
    "\n",
    "            p1 = torch.prod(sigmasquare_2)\n",
    "            p2 = torch.prod(sigmasquare_1)\n",
    "            if p1 == 0 or p2 ==0:\n",
    "                first = 0\n",
    "            else:\n",
    "                first = p1.log() - p2.log()\n",
    "\n",
    "            #first = c_Sigma_2.det().log() - c_Sigma_1.det().log()\n",
    "            second = -dim\n",
    "            third = torch.matmul(c_Sigma_2.inverse(), c_Sigma_1).trace()\n",
    "            fourth = torch.matmul(torch.matmul((c_mu_2 - c_mu_1).T, c_Sigma_2.inverse()), c_mu_2 - c_mu_1)\n",
    "            kl = 0.5 * (first + second + third + fourth)\n",
    "            kkll[x,y] = kl\n",
    "    return kkll\n",
    "\n",
    "n = BATCH_SIZE\n",
    "Kl_matric = np.ones([n,n])\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    print('epoch:', epoch)\n",
    "    for batch in tqdm(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        list_image,list_txt = batch #list_images is list of image in numpy array(np.uint8), or list of PIL images\n",
    "        # print(list_image.size()) #torch.Size([32, 3, 224, 224])\n",
    "        #print(len(list_txt))\n",
    "        images = torch.tensor(np.stack(list_image),requires_grad=True).to(device)\n",
    "        #print('image size:',images.size()) #image size: torch.Size([32, 3, 224, 224])\n",
    "        texts = clip.tokenize(list_txt).to(device) #torch.Size([32, 77])\n",
    "        #  print(texts.size()) #torch.Size([32, 77])\n",
    "        image_u,image_std,text_u,text_std= model_g(images, texts)\n",
    "        #print(image_u.size()) #torch.Size([32, 512])\n",
    "\n",
    "        p_q_kl = loss_multivariate_normal_kl2(image_u,image_std,text_u,text_std)\n",
    "        #print(p_q_kl.size())\n",
    "        # print(Kl_matric.requires_grad)\n",
    "        logits_per_image = torch.tensor(p_q_kl,requires_grad=True).to(device)\n",
    "        logits_per_text = logits_per_image.t()\n",
    "        #print(logits_per_image.size(),logits_per_text.size())\n",
    "\n",
    "        ground_truth = torch.arange(BATCH_SIZE,dtype=torch.long,device=device)\n",
    "        #print(ground_truth.size())\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        total_loss.backward()\n",
    "        print('total loss:', total_loss)\n",
    "      \n",
    "        #convert_models_to_fp32(model)\n",
    "        optimizer.step()\n",
    "        #clip.model.convert_weights(model)\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model_g.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': total_loss,\n",
    "    }, f\"model_checkpoint/model_10.pt\") #just change to your preferred folder/filename   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor(-2.0379)\n",
      "b: tensor(28.0128)\n",
      "c: tensor(18.5910)\n",
      "n 4\n",
      "tensor(20.2830)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "n = 1 #batch size\n",
    "\n",
    "image_u = torch.tensor([0.2, 0.1, 0.5, 0.4])\n",
    "image_std= torch.tensor([0.14, 0.52, 0.2, 0.4])\n",
    "\n",
    "text_u = torch.tensor([0.3, 0.6, -0.5, -0.8])\n",
    "text_std = torch.tensor([0.24, 0.02, 0.31, 0.51])\n",
    "\n",
    "def multivar_continue_KL_divergence2(mu1, Sigma1,mu2, Sigma2):\n",
    "    # print(q[0].shape,q[1].shape)\n",
    "    # print(p[0].shape,p[1].shape)\n",
    "    a = torch.log(torch.det(Sigma2)/torch.det(Sigma1)) \n",
    "    print('a:',a)\n",
    "    b =torch.matmul(torch.inverse(Sigma2), Sigma1).trace()\n",
    "    print('b:',b) \n",
    "    c = torch.matmul(torch.matmul((mu2 - mu1).t(), torch.inverse(Sigma2)), (mu2 - mu1))\n",
    "    print('c:',c)\n",
    "    n = Sigma1.size()[0]\n",
    "    print('n',n)\n",
    "    return 0.5 * (a - n + b + c)\n",
    "\n",
    "\n",
    "# p = (mu1, Sigma1) = torch.transpose(image_u,-1,0), torch.diag_embed(image_std)\n",
    "p = (mu1, Sigma1) = image_u.t(), torch.diag_embed(image_std)\n",
    "\n",
    "# q = (mu2, Sigma2) = torch.transpose(text_u,-1,0), torch.diag_embed(text_std)\n",
    "q = (mu2, Sigma2) = text_u.t(), torch.diag_embed(text_std)\n",
    "\n",
    "print(multivar_continue_KL_divergence2(mu1, Sigma1,mu2, Sigma2))  # 20.28295597572157\n",
    "# print(multivar_continue_KL_divergence(q, p))  # 5.883921991346153"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512])\n",
      "tensor([[1340.9336,  938.8438, 5507.3130,  706.9279],\n",
      "        [1225.6526,  877.7920, 3227.4597,  717.4406],\n",
      "        [1719.1436, 1162.5123, 4957.9834,  649.7206],\n",
      "        [1188.8816,  983.1750, 3340.1199,  670.3173]])\n",
      "tensor([[1331.6353,  961.6362, 5491.2939,  726.7874],\n",
      "        [1219.8347,  904.0649, 3214.9216,  740.7805],\n",
      "        [1693.1235, 1168.5833, 4925.2441,  652.8585],\n",
      "        [1180.8872, 1007.2716, 3325.4053,  691.4807]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "n = 4 #batch size\n",
    "\n",
    "mu1 = torch.rand(n, 512)\n",
    "print(mu1.shape)\n",
    "Sigma1=torch.rand(n, 512)\n",
    "\n",
    "mu2 = torch.rand(n,512)\n",
    "Sigma2=torch.rand(n,512)\n",
    "\n",
    "# mu1 = torch.tensor([[0.2, 0.1, 0.5, 0.4],[0.2, 0.1, 0.5, 0.4]])#.unsqueeze(0)\n",
    "# print(mu1.shape)\n",
    "# Sigma1= torch.tensor([[0.14, 0.52, 0.2, 0.4],[0.14, 0.52, 0.2, 0.4]])#.unsqueeze(0)\n",
    "\n",
    "# mu2 = torch.tensor([[0.3, 0.6, -0.5, -0.8],[0.3, 0.6, -0.5, -0.8]])#.unsqueeze(0)\n",
    "# Sigma2 = torch.tensor([[0.24, 0.02, 0.31, 0.51],[0.24, 0.02, 0.31, 0.51]])#.unsqueeze(0)\n",
    "\n",
    "\n",
    "#Method 2\n",
    "def loss_multivariate_normal_kl2(mu_1, sigmasquare_1, mu_2, sigmasquare_2):\n",
    "    b, dim = mu_1.shape\n",
    "    kl = 0\n",
    "    kkll = torch.ones(b,b)\n",
    "    for x in range(b):\n",
    "        for y in range(b):\n",
    "            c_mu_1 = mu_1[x]\n",
    "            c_Sigma_1 = torch.diag(sigmasquare_1[x])\n",
    "\n",
    "            c_mu_2 = mu_2[y]\n",
    "            c_Sigma_2 = torch.diag(sigmasquare_2[y])\n",
    "\n",
    "            p1 = torch.prod(sigmasquare_2)\n",
    "            p2 = torch.prod(sigmasquare_1)\n",
    "            if p1 == 0 or p2 ==0:\n",
    "                first = 0\n",
    "            else:\n",
    "                first = p1.log() - p2.log()\n",
    "\n",
    "            #first = c_Sigma_2.det().log() - c_Sigma_1.det().log()\n",
    "            second = -dim\n",
    "            third = torch.matmul(c_Sigma_2.inverse(), c_Sigma_1).trace()\n",
    "            fourth = torch.matmul(torch.matmul((c_mu_2 - c_mu_1).T, c_Sigma_2.inverse()), c_mu_2 - c_mu_1)\n",
    "            kl = 0.5 * (first + second + third + fourth)\n",
    "            kkll[x,y] = kl\n",
    "    return kkll\n",
    "\n",
    "#Method 3\n",
    "def loss_multivariate_normal_kl3(mu_1, sigmasquare_1, mu_2, sigmasquare_2):\n",
    "    b, dim = mu_1.shape\n",
    "    kl = 0\n",
    "\n",
    "    c_mu_1=mu_1.unsqueeze(1)\n",
    "    Sigma1=sigmasquare_1.unsqueeze(1)\n",
    "\n",
    "    c_mu_2=mu_2.unsqueeze(0)\n",
    "    Sigma2=sigmasquare_2.unsqueeze(0)\n",
    "\n",
    "    # for x in range(b):\n",
    "    #     for y in range(b):\n",
    "\n",
    "    c_Sigma_1 = torch.diag(sigmasquare_1).unsqueeze(1)\n",
    "    c_Sigma_2 = torch.diag(sigmasquare_2).unsqueeze(0)\n",
    "\n",
    "    p1 = torch.prod(Sigma1)\n",
    "    p2 = torch.prod(Sigma2)\n",
    "    if p1 == 0 or p2 ==0:\n",
    "        first = 0\n",
    "    else:\n",
    "        first = p1.log() - p2.log()\n",
    "\n",
    "    #first = c_Sigma_2.det().log() - c_Sigma_1.det().log()\n",
    "    second = -dim\n",
    "    third = torch.matmul(c_Sigma_2.inverse(), c_Sigma_1).trace()\n",
    "    fourth = torch.matmul(torch.matmul((c_mu_2 - c_mu_1).T, c_Sigma_2.inverse()), c_mu_2 - c_mu_1)\n",
    "    kl = 0.5 * (first + second + third + fourth)\n",
    "\n",
    "    return kl\n",
    "\n",
    "\n",
    "\n",
    "print(loss_multivariate_normal_kl2(mu1,Sigma1,mu2,Sigma2))\n",
    "\n",
    "mu1=mu1.unsqueeze(1)\n",
    "Sigma1=Sigma1.unsqueeze(1)\n",
    "\n",
    "mu2=mu2.unsqueeze(0)\n",
    "Sigma2=Sigma2.unsqueeze(0)\n",
    "\n",
    "#Method 2\n",
    "p_distribution = torch.distributions.MultivariateNormal(mu1, torch.diag_embed(Sigma1))\n",
    "q_distribution = torch.distributions.MultivariateNormal(mu2, torch.diag_embed(Sigma2))\n",
    "p_q_kl = torch.distributions.kl_divergence(p_distribution, q_distribution)#.mean()\n",
    "print(p_q_kl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4\n",
      "tensor(20.2830)\n",
      "tensor([20.2830])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "n = 4 #batch size\n",
    "\n",
    "\n",
    "mu1 = torch.tensor([0.2, 0.1, 0.5, 0.4]).unsqueeze(0)\n",
    "Sigma1= torch.tensor([0.14, 0.52, 0.2, 0.4]).unsqueeze(0)\n",
    "\n",
    "mu2 = torch.tensor([0.3, 0.6, -0.5, -0.8]).unsqueeze(0)\n",
    "Sigma2 = torch.tensor([0.24, 0.02, 0.31, 0.51]).unsqueeze(0)\n",
    "\n",
    "#Method 1 \n",
    "def loss_multivariate_normal_kl(mu_1, sigmasquare_1, mu_2, sigmasquare_2):\n",
    "    b, dim = mu_1.shape\n",
    "    print(b,dim)\n",
    "    kl = 0\n",
    "\n",
    "    for bidx in range(b):\n",
    "        c_mu_1 = mu_1[bidx]\n",
    "        c_Sigma_1 = torch.diag(sigmasquare_1[bidx])\n",
    "\n",
    "        c_mu_2 = mu_2[bidx]\n",
    "        c_Sigma_2 = torch.diag(sigmasquare_2[bidx])\n",
    "\n",
    "        first = c_Sigma_2.det().log() - c_Sigma_1.det().log()\n",
    "        second = -dim\n",
    "        third = torch.matmul(c_Sigma_2.inverse(), c_Sigma_1).trace()\n",
    "        fourth = torch.matmul(torch.matmul((c_mu_2 - c_mu_1).T, c_Sigma_2.inverse()), c_mu_2 - c_mu_1)\n",
    "        kl = kl + 0.5 * (first + second + third + fourth)\n",
    "\n",
    "    kl = kl / b\n",
    "    return kl\n",
    "print(loss_multivariate_normal_kl(mu1,Sigma1,mu2,Sigma2))\n",
    "\n",
    "#Method 2\n",
    "p_distribution = torch.distributions.MultivariateNormal(mu1, torch.diag_embed(Sigma1))\n",
    "q_distribution = torch.distributions.MultivariateNormal(mu2, torch.diag_embed(Sigma2))\n",
    "p_q_kl = torch.distributions.kl_divergence(p_distribution, q_distribution)#.mean()\n",
    "print(p_q_kl)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c73bc775c6f94d98a067ce096eff928d580e9c541aafc395dafbb8814a34bdf4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('torch171': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "image_path='/home/jason/data/coco/images/'\n",
    "mode='train2014/'\n",
    "image_list=[]\n",
    "image_list.extend(glob.glob(os.path.join(image_path,mode, '*.jpg')))\n",
    "image_list.sort()\n",
    "print(len(image_list))\n",
    "\n",
    "text_path='/home/jason/data/coco/text/'\n",
    "label_list = []\n",
    "label_list.extend(glob.glob(os.path.join(text_path,mode, '*.txt')))\n",
    "label_list.sort()\n",
    "print(len(label_list))\n",
    "\n",
    "with open(label_list[0], \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    label = random.choice(data)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69793ba626dd4567a95e1a40d5add3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/323 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "total loss: tensor(1.1172, device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "256\n",
      "total loss: tensor(1.0273, device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "256\n",
      "total loss: tensor(1.2695, device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "256\n",
      "total loss: tensor(1.0596, device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "256\n",
      "total loss: tensor(1.1523, device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "256\n",
      "total loss: tensor(1.1348, device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "256\n",
      "total loss: tensor(1.1523, device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "256\n",
      "total loss: tensor(1.2656, device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "256\n",
      "total loss: tensor(1.2090, device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "256\n",
      "total loss: tensor(1.2129, device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "256\n",
      "total loss: tensor(1.0234, device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>)\n",
      "256\n",
      "total loss: tensor(1.2031, device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jason/CLIP/3_finetune.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 70>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000001vscode-remote?line=85'>86</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtotal loss:\u001b[39m\u001b[39m'\u001b[39m, total_loss)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000001vscode-remote?line=87'>88</a>\u001b[0m         convert_models_to_fp32(model)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000001vscode-remote?line=88'>89</a>\u001b[0m         optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000001vscode-remote?line=89'>90</a>\u001b[0m         clip\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconvert_weights(model)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000001vscode-remote?line=91'>92</a>\u001b[0m torch\u001b[39m.\u001b[39msave({\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000001vscode-remote?line=92'>93</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m: epoch,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000001vscode-remote?line=93'>94</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmodel_state_dict\u001b[39m\u001b[39m'\u001b[39m: model\u001b[39m.\u001b[39mstate_dict(),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000001vscode-remote?line=94'>95</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39moptimizer_state_dict\u001b[39m\u001b[39m'\u001b[39m: optimizer\u001b[39m.\u001b[39mstate_dict(),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000001vscode-remote?line=95'>96</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m: total_loss,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP/3_finetune.ipynb#ch0000001vscode-remote?line=96'>97</a>\u001b[0m     }, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodel_checkpoint/model_10.pt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=22'>23</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/adam.py:108\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/adam.py?line=104'>105</a>\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/adam.py?line=106'>107</a>\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m--> <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/adam.py?line=107'>108</a>\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/adam.py?line=108'>109</a>\u001b[0m            grads,\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/adam.py?line=109'>110</a>\u001b[0m            exp_avgs,\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/adam.py?line=110'>111</a>\u001b[0m            exp_avg_sqs,\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/adam.py?line=111'>112</a>\u001b[0m            max_exp_avg_sqs,\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/adam.py?line=112'>113</a>\u001b[0m            state_steps,\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/adam.py?line=113'>114</a>\u001b[0m            group[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/adam.py?line=114'>115</a>\u001b[0m            beta1,\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/adam.py?line=115'>116</a>\u001b[0m            beta2,\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/adam.py?line=116'>117</a>\u001b[0m            group[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/adam.py?line=117'>118</a>\u001b[0m            group[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/adam.py?line=118'>119</a>\u001b[0m            group[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/adam.py?line=119'>120</a>\u001b[0m            )\n\u001b[1;32m    <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/adam.py?line=120'>121</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/functional.py:87\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/functional.py?line=84'>85</a>\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/functional.py?line=85'>86</a>\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m---> <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/functional.py?line=86'>87</a>\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39;49maddcmul_(grad, grad, value\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta2)\n\u001b[1;32m     <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/functional.py?line=87'>88</a>\u001b[0m \u001b[39mif\u001b[39;00m amsgrad:\n\u001b[1;32m     <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/functional.py?line=88'>89</a>\u001b[0m     \u001b[39m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/jason/miniconda3/envs/torch171/lib/python3.8/site-packages/torch/optim/functional.py?line=89'>90</a>\u001b[0m     torch\u001b[39m.\u001b[39mmaximum(max_exp_avg_sq, exp_avg_sq, out\u001b[39m=\u001b[39mmax_exp_avg_sq)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#https://github.com/openai/CLIP/issues/57\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "EPOCH =10\n",
    "BATCH_SIZE =256\n",
    "\n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "class cocodtrain(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_path='/home/jason/data/coco/images', text_path='/home/jason/data/coco/text', mode='train2014'):\n",
    "\n",
    "        self.image_list = []\n",
    "        self.image_list.extend(glob.glob(os.path.join(image_path, mode, '*.jpg')))\n",
    "        self.image_list.sort()\n",
    "\n",
    "        self.label_list = []\n",
    "        self.label_list.extend(glob.glob(os.path.join(text_path, mode, '*.txt')))\n",
    "        self.label_list.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_list[index]).convert(\"RGB\")\n",
    "        image = image.resize((224,224), Image.BILINEAR)\n",
    "        image = preprocess(image)\n",
    "        #image = np.asarray(image)\n",
    "\n",
    "        with open(self.label_list[index], \"r\") as f:\n",
    "            data = f.readlines()\n",
    "            label = random.choice(data)\n",
    "            \n",
    "        return image, label\n",
    "trainset = cocodtrain('/home/jason/data/coco/images','/home/jason/data/coco/text','train2014')\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "                    trainset, \n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=True, \n",
    "                    num_workers=16,\n",
    "                    drop_last=True)\n",
    "\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "#device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "#model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "#clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    print('epoch:', epoch)\n",
    "    for batch in tqdm(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        list_image,list_txt = batch #list_images is list of image in numpy array(np.uint8), or list of PIL images\n",
    "        # print(list_image.size()) #torch.Size([32, 3, 224, 224])\n",
    "        print(len(list_txt))\n",
    "      \n",
    "        images = torch.tensor(np.stack(list_image)).to(device)\n",
    "        texts = clip.tokenize(list_txt).to(device) #torch.Size([32, 77])\n",
    "         # print(texts.size()) #torch.Size([32, 77])\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "        ground_truth = torch.arange(BATCH_SIZE,dtype=torch.long,device=device)\n",
    "\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        total_loss.backward()\n",
    "        print('total loss:', total_loss)\n",
    "      \n",
    "        convert_models_to_fp32(model)\n",
    "        optimizer.step()\n",
    "        clip.model.convert_weights(model)\n",
    "    \n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': total_loss,\n",
    "    }, f\"model_checkpoint/model_10.pt\") #just change to your preferred folder/filename      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "checkpoint = torch.load(\"model_checkpoint/model_10.pt\")\n",
    "#print(model.input_resolution)\n",
    "# Use these 3 lines if you use default model setting(not training setting) of the clip. For example, if you set context_length to 100 since your string is very long during training, then assign 100 to checkpoint['model_state_dict'][\"context_length\"] \n",
    "# checkpoint['model_state_dict'][\"input_resolution\"] = model.input_resolution #default is 224\n",
    "# checkpoint['model_state_dict'][\"context_length\"] = model.context_length # default is 77\n",
    "# checkpoint['model_state_dict'][\"vocab_size\"] = model.vocab_size \n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clip.model import Gauss_model\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model_g = Gauss_model().to(device)\n",
    "\n",
    "trainset = cocodtrain('/home/jason/data/coco/images','/home/jason/data/coco/text','train2014')\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "                    trainset, \n",
    "                    batch_size=32,\n",
    "                    shuffle=True, \n",
    "                    num_workers=0,\n",
    "                    drop_last=True)\n",
    "\n",
    "for epoch in range(1): \n",
    "        for i, (image, text) in enumerate(tqdm(trainloader)):\n",
    "                print(i)\n",
    "                print(image.size())\n",
    "                print(text.size())\n",
    "                #image_features,text_features = model_g(image,text)\n",
    "                \n",
    "      "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c73bc775c6f94d98a067ce096eff928d580e9c541aafc395dafbb8814a34bdf4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('torch171': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
